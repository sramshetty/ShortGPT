{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from llama import Llama\n",
    "\n",
    "from short_llama import ShortLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaen\\anaconda3\\envs\\shortgpt\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for pg19 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/pg19\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"pg19\", split=\"validation\")  # authors sample 10,000 texts to compute block influences\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator(device=\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and Wrap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaen\\anaconda3\\envs\\shortgpt\\lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 10.96 seconds\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 1024  # authors use a context width of 1024\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=\"../../llama/llama-2-7b\",\n",
    "    tokenizer_path=\"../../llama/tokenizer.model\",\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    max_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x TransformerBlock(\n",
       "    (attention): Attention(\n",
       "      (wq): ColumnParallelLinear()\n",
       "      (wk): ColumnParallelLinear()\n",
       "      (wv): ColumnParallelLinear()\n",
       "      (wo): RowParallelLinear()\n",
       "    )\n",
       "    (feed_forward): FeedForward(\n",
       "      (w1): ColumnParallelLinear()\n",
       "      (w2): RowParallelLinear()\n",
       "      (w3): ColumnParallelLinear()\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ffn_norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama = ShortLlama(llama=llama, n_prune_layers=9)\n",
    "\n",
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': '1960s-70s era pop music. I grew up listening to the radio'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample generation\n",
    "short_llama.llama.text_completion(\n",
    "    prompts=[\"I am an avid fan of \"],\n",
    "    max_gen_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf50ed0464aa454386d996e71b4541b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    prompt_tokens = [short_llama.llama.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "\n",
    "    # authors use a sliding window of size 1024 with a shift of 256\n",
    "    for start in range(0, max_prompt_len, 256):\n",
    "\n",
    "        inputs = [p[start:start+MAX_SEQ_LEN] for p in prompt_tokens if len(p) > start]\n",
    "\n",
    "        short_llama.eval_importance(\n",
    "            prompt_tokens=inputs,\n",
    "            max_gen_len=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8358921.716796875,\n",
       " 5211709.220703125,\n",
       " 3259066.66796875,\n",
       " 3164092.5087890625,\n",
       " 3518517.248046875,\n",
       " 3153696.0009765625,\n",
       " 3062620.751953125,\n",
       " 2856062.2998046875,\n",
       " 2674124.23828125,\n",
       " 2545894.03125,\n",
       " 2382950.501953125,\n",
       " 2194983.1455078125,\n",
       " 2146358.5107421875,\n",
       " 2180816.779296875,\n",
       " 2145900.15234375,\n",
       " 2126212.3974609375,\n",
       " 2180678.5244140625,\n",
       " 1686190.7548828125,\n",
       " 1524035.5732421875,\n",
       " 1270041.162109375,\n",
       " 1368594.52734375,\n",
       " 954588.056640625,\n",
       " 944560.7900390625,\n",
       " 780482.943359375,\n",
       " 743930.5283203125,\n",
       " 732873.1806640625,\n",
       " 745402.265625,\n",
       " 733417.81640625,\n",
       " 762292.994140625,\n",
       " 771143.9541015625,\n",
       " 1303522.251953125,\n",
       " 5824847.5546875]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unimportant layers\n",
    "\n",
    "Layers removed when using pg19 val set: [25, 27, 24, 26, 28, 29, 23, 22, 21]\n",
    "\n",
    "Note: Different order than paper but same 9 least important layers -> [27, 26, 25, 28, 24, 29, 23, 21, 22]\n",
    "\n",
    "Additionally, authors mention that the layer order is quite nuanced and can vary with different datasets. However, relative order suggests similar importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 24, 26, 28, 29, 23, 22, 21]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.remove_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-22): 23 x TransformerBlock(\n",
       "    (attention): Attention(\n",
       "      (wq): ColumnParallelLinear()\n",
       "      (wk): ColumnParallelLinear()\n",
       "      (wv): ColumnParallelLinear()\n",
       "      (wo): RowParallelLinear()\n",
       "    )\n",
       "    (feed_forward): FeedForward(\n",
       "      (w1): ColumnParallelLinear()\n",
       "      (w2): RowParallelLinear()\n",
       "      (w3): ColumnParallelLinear()\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ffn_norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the paper states: \\\n",
    "    - \"Our experiments reveal that the effect of layer removal is significantly more pronounced on generative\n",
    "        tasks compared to multiple-choice tasks. On benchmarks such as GSM8K (Cobbe et al., 2021) and\n",
    "        HumanEval (Chen et al., 2021), removing 25% of the layers often leads to a severe performance\n",
    "        drop, with scores approaching zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': 'Đo n Khơ 20th Century. Hinweis: In = ,t and lồ'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.text_completion(\n",
    "    prompts=[\"I am an avid fan of \"],\n",
    "    max_gen_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Angular Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae0be70aa9344edbd252648c84e08e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    prompt_tokens = [short_llama.llama.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "\n",
    "    # authors use a sliding window of size 1024 with a shift of 256\n",
    "    for start in range(0, max_prompt_len, 256):\n",
    "\n",
    "        inputs = [p[start:start+MAX_SEQ_LEN] for p in prompt_tokens if len(p) > start]\n",
    "\n",
    "        short_llama.eval_importance(\n",
    "            prompt_tokens=inputs,\n",
    "            max_gen_len=0,\n",
    "            angular=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8640.460205078125,\n",
       " 7881.541015625,\n",
       " 7303.3876953125,\n",
       " 7156.226318359375,\n",
       " 7003.533935546875,\n",
       " 6749.5189208984375,\n",
       " 6630.6031494140625,\n",
       " 6494.6051025390625,\n",
       " 6475.490295410156,\n",
       " 6482.81884765625,\n",
       " 6489.277587890625,\n",
       " 6479.0064697265625,\n",
       " 6486.2188720703125,\n",
       " 6440.6580810546875,\n",
       " 6338.8604736328125,\n",
       " 6196.098876953125,\n",
       " 6014.3204345703125,\n",
       " 5677.5113525390625,\n",
       " 5532.0673828125,\n",
       " 5384.6334228515625,\n",
       " 5314.61669921875,\n",
       " 5176.587646484375,\n",
       " 5425.315673828125,\n",
       " 7029.1893310546875,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unimportant layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 22, 23, 24, 25, 26, 27, 28, 29]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.remove_layers(angular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-22): 23 x TransformerBlock(\n",
       "    (attention): Attention(\n",
       "      (wq): ColumnParallelLinear()\n",
       "      (wk): ColumnParallelLinear()\n",
       "      (wv): ColumnParallelLinear()\n",
       "      (wo): RowParallelLinear()\n",
       "    )\n",
       "    (feed_forward): FeedForward(\n",
       "      (w1): ColumnParallelLinear()\n",
       "      (w2): RowParallelLinear()\n",
       "      (w3): ColumnParallelLinear()\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ffn_norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': 'Đo n Khơ 20th Century. Hinweis: In = ,t and lồ'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_llama.llama.text_completion(\n",
    "    prompts=[\"I am an avid fan of \"],\n",
    "    max_gen_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
